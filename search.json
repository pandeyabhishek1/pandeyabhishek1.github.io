[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abhishek Pandey",
    "section": "",
    "text": "This is my personal website for EPPS 6356 (Data Visualization) https://catalog.utdallas.edu/2024/graduate/courses/epps6356."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sub1.html",
    "href": "sub1.html",
    "title": "Untitled",
    "section": "",
    "text": "x &lt;- rnorm(50)\ny &lt;- rnorm(50)\nplot(x, y, main = \"Scatter Plot\", pch = 19)"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Question 1. Try Anscombe’s examples (anscombe01.R on Teams)\nHere is the output chart’s from Anscombe’s examples.\n     \nQuestion 2. Google “generative art”. Cite some examples.\nHere are some of the generative art.\n\n\n\nGenerative Architecture by Michael Hansmeyer\n\n\n\n\n\nMuqarnas by Michael Hansmeyer\n\n\n\n\n\nPlatonic Solids by Michael Hansmeyer\n\n\n\n\n\nSprawl by Mark J Stock\n\n\nQuestion 3: Run Fall.R (on Teams) a. Give your own colors (e.g. Winter). b. Export the file and post on your GitHub website.\n\n\n\nDark Green Coloured Generative Art Tree\n\n\nQuestion 4: Write a critique on a chart in published work (book/article/news website) (Hint: Learn from Nathan Yau’s example discussed in class). Post on your website.\n\n\n\nGNH index by gender chart from World Happiness Report\n\n\nThe title “GNH index by gender” is clear and informative. However, the chart lacks a clear context beyond the brief title. There’s no accompanying text or annotation to explain why these differences in GNH index exist or why they are significant. The colors chosen (pink for Female, orange for National, blue for Male) are distinct and help differentiate the categories. However, the chart would benefit from a more standardized color scheme, especially one that is colorblind-friendly.\nThe use of a 3D bar chart distorts the perception of the data. The 3D perspective can make it difficult to accurately compare the heights of the bars, especially since the tops of the bars are not aligned with the background gridlines. This visual distortion can mislead viewers about the actual differences between the GNH indices. A 2D bar chart or even a simple line graph would likely be more effective for this type of data. The 3D design adds unnecessary complexity without providing additional insight.\nWhile the chart does not appear to intentionally mislead, notice the axis that starts at 0.66. The graph shouldn’t have that because length is the visual cue here, and it makes the differences look greater than they are. In addition, there’s no visible citation or reference to the source of the data. Including the data source would strengthen the chart’s credibility.\nA simple 2D bar chart would improve readability and accuracy, allowing viewers to more easily compare the GNH indices. Also, adding a citation for the data source would enhance the chart’s credibility and transparency. Furthermore, using colors that are distinguishable to those with color vision deficiencies, and ensure that all essential information is accessible without relying solely on color."
  },
  {
    "objectID": "Class5.html",
    "href": "Class5.html",
    "title": "Class-5",
    "section": "",
    "text": "Big data analysis, while offering immense potential, comes with significant challenges that can undermine its effectiveness if not properly addressed. One of the primary pitfalls is the quality of data. Large datasets often contain inaccuracies, inconsistencies, missing values, or outdated information, which can distort the results of analysis if not meticulously cleaned and validated. This can lead to erroneous insights or misguided decisions, especially if analysts assume that the sheer volume of data equates to accuracy. Additionally, integrating data from disparate sources can introduce further complexity, as different systems and formats may not align seamlessly. Without rigorous data governance practices, the analysis may be compromised by poor data quality, rendering any conclusions less reliable.\nAnother key challenge in big data analysis is the risk of overfitting and misinterpretation. Overfitting occurs when models become overly complex, tailoring themselves too closely to the specific dataset used for training, which reduces their applicability to new or broader data. This can result in predictions that perform well in controlled environments but fail in real-world situations. Moreover, the complexity of big data can overwhelm analysts, increasing the risk of misinterpreting trends or relationships within the data. Over-reliance on algorithms and quantitative methods can also lead to ignoring important qualitative or contextual factors that may not be easily captured in the data but are critical for decision-making. These challenges highlight the need for not only advanced analytical tools but also skilled interpretation to avoid drawing inaccurate or biased conclusions from big data.\n\n\n\nOverfitting is a common challenge in data analysis and machine learning, occurring when a model becomes too complex and overly tailored to the specific dataset it is trained on. This happens when the model captures not only the underlying patterns but also the noise and random fluctuations in the data. As a result, while the model may perform exceptionally well on the training data, it struggles to generalize to new, unseen data, leading to poor predictive performance in real-world applications. Overfitting often arises when a model is exposed to too many variables or features, and this excess complexity allows it to memorize the data rather than learning its fundamental structure. Techniques such as cross-validation, regularization, and simplifying the model can help mitigate the risk of overfitting by ensuring that it captures only the most relevant patterns.\nClosely related to overfitting is the issue of overparameterization, which occurs when a model contains too many parameters relative to the amount of data available. In overparameterized models, the model’s flexibility increases, allowing it to fit the training data almost perfectly, even if the data contains noise or anomalies. However, this can lead to models that are excessively sensitive to small changes in the data, resulting in poor generalization and unstable predictions. Overparameterization is particularly problematic in deep learning and neural networks, where the number of parameters can easily exceed the amount of meaningful data available for training. To avoid overparameterization, it is crucial to balance model complexity with the size and quality of the dataset, using techniques like dimensionality reduction, pruning, and regularization to ensure that the model remains robust and generalizable."
  },
  {
    "objectID": "Class5.html#short-notes-on",
    "href": "Class5.html#short-notes-on",
    "title": "Class-5",
    "section": "",
    "text": "Big data analysis, while offering immense potential, comes with significant challenges that can undermine its effectiveness if not properly addressed. One of the primary pitfalls is the quality of data. Large datasets often contain inaccuracies, inconsistencies, missing values, or outdated information, which can distort the results of analysis if not meticulously cleaned and validated. This can lead to erroneous insights or misguided decisions, especially if analysts assume that the sheer volume of data equates to accuracy. Additionally, integrating data from disparate sources can introduce further complexity, as different systems and formats may not align seamlessly. Without rigorous data governance practices, the analysis may be compromised by poor data quality, rendering any conclusions less reliable.\nAnother key challenge in big data analysis is the risk of overfitting and misinterpretation. Overfitting occurs when models become overly complex, tailoring themselves too closely to the specific dataset used for training, which reduces their applicability to new or broader data. This can result in predictions that perform well in controlled environments but fail in real-world situations. Moreover, the complexity of big data can overwhelm analysts, increasing the risk of misinterpreting trends or relationships within the data. Over-reliance on algorithms and quantitative methods can also lead to ignoring important qualitative or contextual factors that may not be easily captured in the data but are critical for decision-making. These challenges highlight the need for not only advanced analytical tools but also skilled interpretation to avoid drawing inaccurate or biased conclusions from big data.\n\n\n\nOverfitting is a common challenge in data analysis and machine learning, occurring when a model becomes too complex and overly tailored to the specific dataset it is trained on. This happens when the model captures not only the underlying patterns but also the noise and random fluctuations in the data. As a result, while the model may perform exceptionally well on the training data, it struggles to generalize to new, unseen data, leading to poor predictive performance in real-world applications. Overfitting often arises when a model is exposed to too many variables or features, and this excess complexity allows it to memorize the data rather than learning its fundamental structure. Techniques such as cross-validation, regularization, and simplifying the model can help mitigate the risk of overfitting by ensuring that it captures only the most relevant patterns.\nClosely related to overfitting is the issue of overparameterization, which occurs when a model contains too many parameters relative to the amount of data available. In overparameterized models, the model’s flexibility increases, allowing it to fit the training data almost perfectly, even if the data contains noise or anomalies. However, this can lead to models that are excessively sensitive to small changes in the data, resulting in poor generalization and unstable predictions. Overparameterization is particularly problematic in deep learning and neural networks, where the number of parameters can easily exceed the amount of meaningful data available for training. To avoid overparameterization, it is crucial to balance model complexity with the size and quality of the dataset, using techniques like dimensionality reduction, pruning, and regularization to ensure that the model remains robust and generalizable."
  },
  {
    "objectID": "Class5.html#name-the-technologiestechniques-wickham-introduced.-what-are-his-main-points.-summarize-and-comment.",
    "href": "Class5.html#name-the-technologiestechniques-wickham-introduced.-what-are-his-main-points.-summarize-and-comment.",
    "title": "Class-5",
    "section": "Name the technologies/techniques Wickham introduced. What are his main points. Summarize and comment.",
    "text": "Name the technologies/techniques Wickham introduced. What are his main points. Summarize and comment.\nHadley Wickham, a key figure in the data science community, is known for his contributions to data visualization, particularly with technologies and techniques like ggplot2, tidyverse, and dplyr in R. His work emphasizes the importance of tidy data, reproducibility, and the systematic transformation of raw data into meaningful visualizations. Wickham advocates for using layered grammar in data visualization, which allows users to build complex plots step by step. His main points likely center on simplifying data handling and making data science more accessible through intuitive, efficient tools.\nThese tools have revolutionized the way data is analyzed and visualized, especially by making complex statistical models easier to understand. Wickham’s focus on usability and reproducibility ensures that even users with limited programming backgrounds can engage deeply with their data, enabling wider participation in data-driven fields."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "##Assignment4Compiled\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nfile_path &lt;- \"C:/Users/pande/Downloads/DV_ProjectData.csv\"  # or \"C:\\\\Users\\\\ams190002\\\\Downloads\\\\DV_ProjectData.csv\"\ndata &lt;- read.csv(file_path)\nhead(data)\nsummary(data)\n\n## Cleaning Data \n\nstate_means &lt;- data %&gt;%\ngroup_by(State) %&gt;%\nsummarise(across(c(VoterTurnoutRate, MedianHouseholdIncome, UnemploymentRate, HSCompletion, SomeCollegeEducation,PoorOrFairHealth, AvgPoorPhysHealthDays, AvgPoorMentalHealthDays, ObesityRate, \nFoodEnviornmentIndex, FoodInsecurity, LimitedAccesstoHealthyFoods, PhysInactivityRate,Population), mean))\nView(state_means)\nstates_of_interest &lt;- c(\"CA\", \"AR\", \"TX\", \"OK\", \"NJ\")\nstate_means_filtered &lt;- state_means %&gt;%\n  filter(State %in% states_of_interest)\n\npar(family=\"serif\")\n\n## Graph 1\n\nbluecolorramp &lt;- colorRampPalette(c(\"skyblue\", \"mediumblue\"))(length(state_means_filtered$MedianHouseholdIncome))\ncolors &lt;- bluecolorramp[rank(state_means_filtered$MedianHouseholdIncome)]\n\npar(family=\"serif\", cex=0.9, mar=c(3, 3.5, 3, 1))\nbarplot(state_means_filtered$VoterTurnoutRate, width=state_means_filtered$MedianHouseholdIncome, space=0, \n        col=colors, ylim=c(0,0.8), names.arg=state_means_filtered$State)\ntitle(main=\"Voter Turnout vs Median Household Income\", cex.main=1.5)\nmtext(\"Voter Turnout Rate\", , side=2, line=2.2, cex=1.1)\nbox(bty=\"l\")\nlegend(\"topright\", legend = c(\"Low Income\", \"High Income\"), fill = c(\"skyblue\", \"mediumblue\"), \n       title = \"Income Level\")\n\n## Graph 2\n\nreduceddata &lt;- data%&gt;%\n  filter(State == \"CA\"| State==\"AR\"|State ==\"TX\" | State == \"OK\"|State ==\"NJ\")\n\nggplot(data = reduceddata,aes(x=VoterTurnoutRate, color = State))+\n  geom_density()+\n  scale_color_brewer(palette = \"Dark2\")+\n  theme_minimal()+\n  theme(legend.position = \"none\",\n        text = element_text(family = \"serif\"))+\n  facet_wrap(~State)\n\n\n## Graph 3\n\n\nggplot(state_means_filtered, aes(x = reorder(State, VoterTurnoutRate), y = VoterTurnoutRate, fill= \"forestgreen\")) + \n  geom_col(show.legend=FALSE) + \n  labs(title = \"Voter Turnout Rate by State\", x = \"State\", y = \"Voter Turnout Rate\") + \n  theme_classic() +\n  theme(text = element_text(family = \"serif\")) +\n  coord_flip()\n\n##Graph 4\n\n# Cleaning Data to Include DFW Counties\ndfw_counties &lt;- c(\"Collin County\", \"Dallas County\", \"Denton County\", \"Ellis County\", \n                  \"Hunt County\", \"Kaufman County\", \"Rockwall County\", \n                  \"Johnson County\", \"Parker County\", \"Tarrant County\", \"Wise County\")\n\ndfw_data &lt;- subset(data, CountyName %in% dfw_counties)\n\n# Convert the data from wide to long format\ndfw_data_long &lt;- dfw_data %&gt;%\n  pivot_longer(cols = c(\"HSCompletion\", \"SomeCollegeEducation\"), \n               names_to = \"EducationLevel\", \n               values_to = \"Percentage\")\n\n# Create the column chart\nggplot(dfw_data_long, aes(x = CountyName, y = Percentage * 100, fill = EducationLevel)) + \n  geom_bar(stat = 'identity', position = 'dodge') + \n  labs(title = 'High School Completion vs Some College Education in DFW Metropolis ',\n       x = 'County Name', y = 'Percentage (%)') + \n  scale_fill_manual(name = 'Education Level', \n                    values = c('HSCompletion' = 'steelblue', 'SomeCollegeEducation' = 'forestgreen')) + \n  theme_minimal() + \n  theme(text = element_text(family = \"serif\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))"
  },
  {
    "objectID": "Class6.html",
    "href": "Class6.html",
    "title": "Class 6",
    "section": "",
    "text": "Edward Tufte and Hadley Wickham represent two distinct philosophies and approaches in the field of data visualization, with each leaving a profound impact on how information is visualized and interpreted. Edward Tufte, an academic and expert on information design, emphasizes clarity and precision above all else, advocating for minimalism in visual displays. Tufte’s approach is governed by his concept of the “data-ink ratio,” which suggests that any visual element that does not represent data should be minimized or eliminated. His designs, often free of “chartjunk” or excessive decoration, aim to present complex data in ways that allow the viewer to discern patterns and insights without distraction. This is evident in Tufte’s notable works, such as The Visual Display of Quantitative Information, where he argues that good design is unobtrusive and that the goal of any data visualization is to provide a clear and accurate representation of data.\nIn contrast, Hadley Wickham, a data scientist and software developer, approaches data visualization through a computational lens, emphasizing flexibility, reproducibility, and the role of visualization in the analytical process. Wickham’s development of the ggplot2 package for the R programming language is grounded in Leland Wilkinson’s “Grammar of Graphics,” a framework that allows users to build complex visualizations layer by layer. This modularity enables users to add components, such as points, lines, or colors, progressively, resulting in visualizations that can be adapted to convey a range of insights from the same data. Wickham’s style is functional and dynamic, prioritizing the ability to explore and manipulate data visually, thus supporting an iterative approach to data analysis where visualizations serve both as tools for discovery and communication.\nBoth Tufte and Wickham prioritize the utility of data visualization but approach it from different perspectives. Tufte’s work is rooted in the aesthetics of design and the cognitive perception of information, focusing on reducing visual clutter to enhance understanding. His approach is ideal for static, well-considered presentations of data. Wickham, on the other hand, emphasizes a data-driven, programmatic approach that aligns with the needs of modern data science, where data visualization is often part of an interactive, iterative process. This makes Wickham’s approach well-suited for exploratory data analysis, particularly in data science, where the ability to experiment with visual structures is critical. Together, Tufte and Wickham highlight the diversity within data visualization, showing that different approaches can be beneficial depending on the purpose, audience, and analytical needs of the visualization."
  },
  {
    "objectID": "Class6.html#short-notes-on",
    "href": "Class6.html#short-notes-on",
    "title": "Class 6",
    "section": "",
    "text": "Edward Tufte and Hadley Wickham represent two distinct philosophies and approaches in the field of data visualization, with each leaving a profound impact on how information is visualized and interpreted. Edward Tufte, an academic and expert on information design, emphasizes clarity and precision above all else, advocating for minimalism in visual displays. Tufte’s approach is governed by his concept of the “data-ink ratio,” which suggests that any visual element that does not represent data should be minimized or eliminated. His designs, often free of “chartjunk” or excessive decoration, aim to present complex data in ways that allow the viewer to discern patterns and insights without distraction. This is evident in Tufte’s notable works, such as The Visual Display of Quantitative Information, where he argues that good design is unobtrusive and that the goal of any data visualization is to provide a clear and accurate representation of data.\nIn contrast, Hadley Wickham, a data scientist and software developer, approaches data visualization through a computational lens, emphasizing flexibility, reproducibility, and the role of visualization in the analytical process. Wickham’s development of the ggplot2 package for the R programming language is grounded in Leland Wilkinson’s “Grammar of Graphics,” a framework that allows users to build complex visualizations layer by layer. This modularity enables users to add components, such as points, lines, or colors, progressively, resulting in visualizations that can be adapted to convey a range of insights from the same data. Wickham’s style is functional and dynamic, prioritizing the ability to explore and manipulate data visually, thus supporting an iterative approach to data analysis where visualizations serve both as tools for discovery and communication.\nBoth Tufte and Wickham prioritize the utility of data visualization but approach it from different perspectives. Tufte’s work is rooted in the aesthetics of design and the cognitive perception of information, focusing on reducing visual clutter to enhance understanding. His approach is ideal for static, well-considered presentations of data. Wickham, on the other hand, emphasizes a data-driven, programmatic approach that aligns with the needs of modern data science, where data visualization is often part of an interactive, iterative process. This makes Wickham’s approach well-suited for exploratory data analysis, particularly in data science, where the ability to experiment with visual structures is critical. Together, Tufte and Wickham highlight the diversity within data visualization, showing that different approaches can be beneficial depending on the purpose, audience, and analytical needs of the visualization."
  },
  {
    "objectID": "Class7.html",
    "href": "Class7.html",
    "title": "Class 7",
    "section": "",
    "text": "1.What is literate programming (Knuth 1984)?\nLiterate programming, introduced by Donald Knuth in 1984, is a programming paradigm that combines code with rich documentation to make software more understandable to humans. In this approach, a program is written as a narrative that includes both the code and explanations of the design, logic, and intent behind it. Rather than writing code solely to be executed by a machine, literate programming treats a program as a readable document, designed for human comprehension. The idea is that code should be accompanied by descriptions that help readers (including the programmer themselves) understand not just what the code does but also why it is structured the way it is. This method makes complex programs more accessible, readable, and maintainable by embedding them within a story-like format that explains both high-level goals and low-level details.\nKnuth’s original implementation of literate programming introduced a tool called WEB, which allowed developers to write programs in a mixture of documentation and code. This tool separated the source code into sections, allowing the writer to describe the program in a logical, human-friendly sequence rather than following the strict, often fragmented order required by a compiler. The source document could then be processed to produce both the executable code and a richly formatted document describing the code. Literate programming influenced many later tools and ideas, including Jupyter Notebooks in data science, which allow for interactive code interspersed with explanatory text and visualizations. By focusing on readability and documentation, literate programming encourages a deeper understanding of software, treating programming as a form of literary composition.\n\n\n2.Watch one of Stephen Malinowski’s videos on music visualization on YouTube and write a one-page review about shape, color and other elements in visualization\nStephen Malinowski’s music visualizations, as seen in his Music Animation Machine project, are renowned for transforming music into an engaging visual experience that is both educational and immersive. His work uses shapes and colors to represent different notes, voices, and rhythms in a piece of music. For instance, in his visualization of Bach’s Passacaglia and Fugue in C minor, Malinowski employs bars of varying lengths to represent the duration and dynamics of notes, making it easier for viewers to follow the musical structure. The colors are not just decorative; they signify different musical lines, which is especially useful in polyphonic music. This color-coding creates a sense of depth, helping viewers track multiple melodies or harmonic progressions simultaneously, even without formal music training.\nIn addition to color, the shapes in Malinowski’s visualizations add another layer of clarity and visual appeal. Each shape corresponds to specific notes or instruments, allowing viewers to “see” the music’s structure and interactions. For example, in Beethoven’s string quartets, each instrument might be represented by a unique shape, such as circles or squares, that travels across the screen in synchronization with the music. This approach enables the audience to appreciate the interplay between instruments and detect patterns or motifs within the piece. By translating these elements into moving visuals, Malinowski’s animations facilitate a near-synesthetic experience, bringing viewers closer to the essence of the composition.\nMalinowski’s style also reflects a broader philosophy of making music more accessible through movement and visual coherence. Unlike traditional static musical notation, which requires training to interpret, his animations move with the music’s rhythm, mirroring its pulse and flow. This method lowers the barrier for musical appreciation, particularly for viewers who may not be familiar with reading sheet music. By creating an intuitive, almost tactile connection between sound and sight, Malinowski has expanded the educational and emotional reach of classical music, offering an innovative, visual pathway for understanding its complexity and beauty. His work is celebrated for making music a multidimensional experience, allowing both novices and experts to explore classical compositions in a refreshing, engaging way.\n\n\n3. What are the Four Pillars of Visualization?\nThe Four Pillars of Visualization emphasize purpose, content, structure, and formatting as the foundation for effective data visualizations:\nPurpose: The purpose clarifies why the visualization exists. Visualizations are generally intended to inform, persuade, or explore data. Knowing this helps decide which data to highlight, the tone of the design, and how much detail to include. For instance, an exploratory graph might include more details than a high-level infographic.\nContent: Content is about selecting only the data that directly supports the visualization’s purpose. Focusing on relevant data helps prevent clutter and confusion, making it easier for viewers to grasp the intended message without distraction from extraneous details.\nStructure: Structure concerns the arrangement of data into a coherent, logical format. Choosing the appropriate layout (e.g., bar chart, scatter plot, line graph) helps make data relationships, trends, and comparisons clear. For example, a line chart is suited for showing trends over time, while pie charts can display proportions.\nFormatting: Formatting involves styling elements, including colors, labels, and fonts, to make the visualization visually appealing and easy to read. Effective formatting directs viewer attention to key data points and maintains accessibility, ensuring clarity. For example, distinct colors or bolded fonts can highlight key findings or trends.\nFor more information, refer to the UC Berkeley and Duke University guide on data visualization."
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "1.The output of Murrel01.R are:\n\n ### 2. The program generating plots similar to murrel01 using happiness dataset is:\n::: {.cell}\n# Load the dataset\ndata &lt;- read_excel(\"C:/Users/pande/OneDrive - The University of Texas at Dallas/PhD/2024_Fall/Data_Visualization/Happiness.xlsx\")\n\n# Set layout for two plots side by side\npar(mfrow = c(1, 2)) \nhist(data$`Life Expectancy (years)`, main = \"Life Expectancy Distribution\", xlab = \"Years\")\nhist(data$`GDP per capita ($)`, main = \"GDP per Capita Distribution\", xlab = \"GDP ($)\")\n\n# Reset layout to a single plot\npar(mfrow = c(1, 1))\n\n# Plot Life Expectancy vs Wellbeing with a smoothed line\nplot(data$`Life Expectancy (years)`, data$`Ladder of life (Wellbeing) (0-10)`, \n     main = \"Life Expectancy vs Wellbeing\", xlab = \"Life Expectancy (years)\", ylab = \"Wellbeing Score\")\nlines(lowess(data$`Life Expectancy (years)`, data$`Ladder of life (Wellbeing) (0-10)`), col = \"blue\") \n\n# Plot Life Expectancy vs HPI with red points overlay\nplot(data$`Life Expectancy (years)`, data$HPI, main = \"Life Expectancy vs HPI\", \n     xlab = \"Life Expectancy (years)\", ylab = \"Happy Planet Index\")\npoints(data$`Life Expectancy (years)`, data$HPI, col = \"red\", pch = 16) \n\n# Plot HPI vs GDP with custom axes\nplot(data$HPI, data$`GDP per capita ($)`, main = \"HPI vs GDP\", \n     xlab = \"Happy Planet Index\", ylab = \"GDP per Capita ($)\", axes = FALSE)\naxis(1, at = seq(0, 100, by = 10))\naxis(2, las = 1)\nbox()\n\n# Histogram of Carbon Footprint with a box around the plot\nhist(data$`Carbon Footprint (tCO2e)`, main = \"Carbon Footprint Distribution\", xlab = \"tCO2e\")\nbox()\n\n# Scatter plot with text labels\nplot(data$`Life Expectancy (years)`, data$`GDP per capita ($)`, \n     main = \"Life Expectancy vs GDP per Capita\", xlab = \"Life Expectancy (years)\", ylab = \"GDP per Capita ($)\")\ntext(data$`Life Expectancy (years)`, data$`GDP per capita ($)`, labels = data$Country, cex = 0.7, pos = 4, col = \"blue\")\n\n# Histogram with mtext annotations\nhist(data$`Ladder of life (Wellbeing) (0-10)`, main = \"Wellbeing Score Distribution\", xlab = \"Wellbeing Score\")\nmtext(\"Wellbeing Histogram\", side = 3, line = 2)\nmtext(\"Sample Data on Global Happiness\", side = 1, line = 3, col = \"red\")\n\n# Histogram for GDP per capita\nhist(data$`GDP per capita ($)`, main = \"Histogram of GDP per Capita\", xlab = \"GDP per Capita ($)\")\n\n# Boxplot of Life Expectancy by Continent\nboxplot(data$`Life Expectancy (years)` ~ data$Continent, main = \"Life Expectancy by Continent\", \n        xlab = \"Continent\", ylab = \"Life Expectancy (years)\")\n\n# Scatter plot with legend\nplot(data$`Life Expectancy (years)`, data$`Ladder of life (Wellbeing) (0-10)`, col = \"blue\", pch = 16, \n     main = \"Life Expectancy vs Wellbeing\")\nlegend(\"topright\", legend = \"Data Points\", col = \"blue\", pch = 16)\n\n# 3D Perspective plot (example with arbitrary z values for illustration)\npersp(x = seq(1, nrow(data)), y = seq(1, nrow(data)), \n      z = outer(data$`Life Expectancy (years)`, data$HPI, \"+\"), \n      main = \"3D Perspective of Life Expectancy and HPI\", xlab = \"Index\", ylab = \"Index\", zlab = \"Sum\")\n\n# Bar plot with names as labels\nbarplot(data$HPI[1:10], names.arg = data$Country[1:10], las = 2, \n        main = \"HPI for Top 10 Countries\", xlab = \"Country\", ylab = \"HPI\")\n\n# Pie chart of population distribution for the top 10 countries\npie(data$`Population (thousands)`[1:10], labels = data$Country[1:10], \n    main = \"Population Distribution (Top 10 Countries)\")\n\n\n\n#::: {.cell-output-display} The output of the program are: \n           \n#:::"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "::: {.cell}\n\n# I selected the last Piechart\n\n# Set graphical parameters: 'mar' changes the margins of the plot, 'xpd' controls clipping, and 'cex' changes the character size\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.5)\n\n# Create a vector representing sales proportions for different pie types\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\n\n# Assign names to the different sections of the pie chart (corresponding to each flavor)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\n\n# Plot the pie chart using the 'pie' function\n# 'col' specifies the colors of the pie sections, using a grayscale gradient created by 'gray()'\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)))"
  },
  {
    "objectID": "Assignment3.html#the-explaination-of-configuration-of-pie-chart-at-the-end-is",
    "href": "Assignment3.html#the-explaination-of-configuration-of-pie-chart-at-the-end-is",
    "title": "Assignment 3",
    "section": "",
    "text": "::: {.cell}\n\n# I selected the last Piechart\n\n# Set graphical parameters: 'mar' changes the margins of the plot, 'xpd' controls clipping, and 'cex' changes the character size\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.5)\n\n# Create a vector representing sales proportions for different pie types\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\n\n# Assign names to the different sections of the pie chart (corresponding to each flavor)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\n\n# Plot the pie chart using the 'pie' function\n# 'col' specifies the colors of the pie sections, using a grayscale gradient created by 'gray()'\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)))"
  },
  {
    "objectID": "Assignment3.html#rerun-the-anscombe02.r-in-tesms-folder",
    "href": "Assignment3.html#rerun-the-anscombe02.r-in-tesms-folder",
    "title": "Assignment 3",
    "section": "2 Rerun the anscombe02.R (in tesms folder)",
    "text": "2 Rerun the anscombe02.R (in tesms folder)\n\na. Compare the regression models.\nAnscombe’s consists of four datasets that have nearly identical statistical properties (means, variances, correlations, and regressions) but show vastly different patterns when plotted. The key takeaways from comparing the four regression models are:\nIdentical Regressions: All four regression models have very similar (if not identical) linear regression lines in terms of slope and intercept. This demonstrates that identical statistical summaries can result from very different underlying data distributions.\nDifferent Distributions:\n\nModel 1 (y1 ~ x1): A standard linear relationship with data points closely clustered around the line.\nModel 2 (y2 ~ x2): A slight curve in the data, indicating a non-linear relationship, but the linear regression still fits closely.\nModel 3 (y3 ~ x3): A linear relationship but with one outlier, which has a significant effect on the regression.\nModel 4 (y4 ~ x4): Most data points are vertically aligned with one extreme outlier. The outlier drives the slope, making the regression line a poor representation of the rest of the data.\n\n\n\nCompare the different ways to create the plots\n\nColors:\nIn the “fancy version,” the points are colored red with a background of orange (col = “red”, bg = “orange”), and the regression lines are blue (abline(mods[[i]], col = “blue”)). You can experiment with other colors to improve clarity, like using darker or contrasting colors for better visibility.\nPlot Characters (pch):\npch = 21 is used, which gives a circle with a filled background. Changing pch will modify the appearance of the points (e.g., pch = 16 for solid circles, pch = 17 for triangles).\nLine Types:\nYou can change the appearance of the regression line by using the lty argument in abline(). For instance, lty = 2 creates a dashed line, while lty = 3 creates a dotted line.\nPoint Size (cex):\nThe size of the points is controlled using the cex parameter. In the example, it is set to 1.2, making the points slightly larger than default.\nAxis Limits (xlim, ylim):\nThe limits of the x and y axes are controlled using the xlim and ylim arguments. In this case, xlim = c(3, 19) and ylim = c(3, 13) ensure that the axis range is consistent across all plots, allowing better comparison."
  },
  {
    "objectID": "Assignment3.html#rerun-the-anscombe01.r-in-tesms-folder",
    "href": "Assignment3.html#rerun-the-anscombe01.r-in-tesms-folder",
    "title": "Assignment 3",
    "section": "2 Rerun the anscombe01.R (in tesms folder)",
    "text": "2 Rerun the anscombe01.R (in tesms folder)\n\n\n\nOutput of anscombe01.R\n\n\n\na. Compare the regression models.\nAnscombe’s consists of four datasets that have nearly identical statistical properties (means, variances, correlations, and regressions) but show vastly different patterns when plotted. The key takeaways from comparing the four regression models are:\nIdentical Regressions: All four regression models have very similar (if not identical) linear regression lines in terms of slope and intercept. This demonstrates that identical statistical summaries can result from very different underlying data distributions.\nDifferent Distributions:\n\nModel 1 (y1 ~ x1): A standard linear relationship with data points closely clustered around the line.\nModel 2 (y2 ~ x2): A slight curve in the data, indicating a non-linear relationship, but the linear regression still fits closely.\nModel 3 (y3 ~ x3): A linear relationship but with one outlier, which has a significant effect on the regression.\nModel 4 (y4 ~ x4): Most data points are vertically aligned with one extreme outlier. The outlier drives the slope, making the regression line a poor representation of the rest of the data.\n\n\n\nCompare the different ways to create the plots\n\nColors:\nIn the “fancy version,” the points are colored red with a background of orange (col = “red”, bg = “orange”), and the regression lines are blue (abline(mods[[i]], col = “blue”)). You can experiment with other colors to improve clarity, like using darker or contrasting colors for better visibility.\nPlot Characters (pch):\npch = 21 is used, which gives a circle with a filled background. Changing pch will modify the appearance of the points (e.g., pch = 16 for solid circles, pch = 17 for triangles).\nLine Types:\nYou can change the appearance of the regression line by using the lty argument in abline(). For instance, lty = 2 creates a dashed line, while lty = 3 creates a dotted line.\nPoint Size (cex):\nThe size of the points is controlled using the cex parameter. In the example, it is set to 1.2, making the points slightly larger than default.\nAxis Limits (xlim, ylim):\nThe limits of the x and y axes are controlled using the xlim and ylim arguments. In this case, xlim = c(3, 19) and ylim = c(3, 13) ensure that the axis range is consistent across all plots, allowing better comparison."
  },
  {
    "objectID": "Assignment3.html#section",
    "href": "Assignment3.html#section",
    "title": "Assignment 3",
    "section": "3",
    "text": "3\n# Set a serif font\npar(family = \"serif\")\n\n# Use a 2x2 layout for the four datasets\npar(mfrow = c(2, 2), oma = c(0, 0, 2, 0), mar = c(4, 4, 1, 1))\n\n# Define custom plot settings and apply them to all four datasets\nfor(i in 1:4) {\n  # Use formula and access columns directly from 'anscombe' dataframe\n  formula &lt;- as.formula(paste0(\"y\", i, \" ~ x\", i))\n  \n  # Plot each x and y pair using anscombe data, with custom axis labels\n  plot(anscombe[[paste0(\"x\", i)]], anscombe[[paste0(\"y\", i)]],\n       col = \"darkgreen\", pch = 17, main = paste(\"Dataset\", i),\n       xlim = c(4, 20), ylim = c(4, 12),\n       xlab = paste(\"x\", i), ylab = paste(\"y\", i))  # Set custom axis labels\n  \n  # Add regression line for each plot\n  abline(lm(formula, data = anscombe), col = \"purple\", lwd = 2)\n}\n\n# Add a common title\nmtext(\"Customized Anscombe's Quartet\", outer = TRUE, cex = 1.5)"
  },
  {
    "objectID": "Assignment3.html#can-you-finetune-the-graph-without-using-other-packages",
    "href": "Assignment3.html#can-you-finetune-the-graph-without-using-other-packages",
    "title": "Assignment 3",
    "section": "3 Can you finetune the graph without using other packages?",
    "text": "3 Can you finetune the graph without using other packages?\n# Set a serif font\npar(family = \"serif\")\n\n# Use a 2x2 layout for the four datasets\npar(mfrow = c(2, 2), oma = c(0, 0, 2, 0), mar = c(4, 4, 1, 1))\n\n# Define custom plot settings and apply them to all four datasets\nfor(i in 1:4) {\n  # Use formula and access columns directly from 'anscombe' dataframe\n  formula &lt;- as.formula(paste0(\"y\", i, \" ~ x\", i))\n  \n  # Plot each x and y pair using anscombe data, with custom axis labels\n  plot(anscombe[[paste0(\"x\", i)]], anscombe[[paste0(\"y\", i)]],\n       col = \"darkgreen\", pch = 17, main = paste(\"Dataset\", i),\n       xlim = c(4, 20), ylim = c(4, 12),\n       xlab = paste(\"x\", i), ylab = paste(\"y\", i))  # Set custom axis labels\n  \n  # Add regression line for each plot\n  abline(lm(formula, data = anscombe), col = \"purple\", lwd = 2)\n}\n\n# Add a common title\nmtext(\"Customized Anscombe's Quartet\", outer = TRUE, cex = 1.5)\n\n\n\nFinetuned Graph without other packages\n\n\n###4 How about with ggplot2? (Use tidyverse package)\n# Load tidyverse for ggplot2 and data manipulation\nlibrary(tidyverse)\n\n# Convert the anscombe dataset to long format\nanscombe_long &lt;- anscombe %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\n# Create the ggplot with facets for the four datasets\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(color = \"darkgreen\", size = 3, shape = 17) +  # Custom color and shape for points\n  geom_smooth(method = \"lm\", color = \"purple\", se = FALSE, linetype = \"dashed\") +  # Regression line\n  facet_wrap(~ set, nrow = 2, ncol = 2) +  # Create a 2x2 grid of plots\n  labs(title = \"Anscombe's Quartet (Customized with ggplot2)\",\n       x = \"X values\", y = \"Y values\") +  # Set axis labels and title\n  theme_minimal() +  # Apply a clean theme\n  theme(text = element_text(family = \"serif\", size = 12),  # Apply a serif font\n        plot.title = element_text(hjust = 0.5))  # Center align the title\n\n\n\n\n\n\nFinetuned Graph with ggplot2 and tidyverse\n\n\n\nPrehackathon by Team: Replicate the scatterplot matrix\n## Download COVID data from OWID GitHub\nowidall &lt;- read.csv(\"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\")\nid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by continent: Europe\nowideu = subset(owidall, continent==\"Europe\" & as.Date(date) &lt; as.Date(\"2023-08-31\") & as.Date(date) &gt; as.Date(\"2020-01-01\"))\nselected_countries &lt;- c(\"Spain\", \"Germany\", \"Italy\", \"Ukraine\")\nowideu &lt;- subset(owideu, location %in% selected_countries)\n\nylim = c(0, 6000)\n\n# font\npar(family = \"serif\")\n\n# y axis: retreived using chatgpt\ny_ticks &lt;- c(1000, 3000, 5000) \n\n# plot- debugged using chatgpt\n\nplot(x = as.Date(owideu$date), y = owideu$new_deaths, pch = 16, col = \"#bf106d\", xaxt = \"n\", yaxt = \"n\", ylab = \"\", xlab = \"\")\naxis.Date(1, at = as.Date(owideu$date), labels = format(as.Date(owideu$date), \"%Y-%m\"), las = 2, cex.axis = 0.6)  # X-axis\naxis(2, at = y_ticks, labels = y_ticks, cex.axis = 0.6)  \n\nmtext(\"Date\", side = 1, line = 4, cex = 1.5)  \nmtext(\"COVID Deaths in Europe (Daily)\", side = 2, line = 2, las = 0, cex = 1.2)"
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "library (readxl)\nhappiness_data &lt;- read_excel(\"C:/Users/pande/OneDrive - The University of Texas at Dallas/PhD/2024_Fall/Data_Visualization/Happiness.xlsx\")\n\n# Group 1: Histogram and Vertical Barchart\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 1. Histogram of Life Expectancy\nhist(happiness_data$`Life Expectancy (years)`, \n     col = \"lightblue\", \n     main = \"Histogram of Life Expectancy\", \n     xlab = \"Life Expectancy (years)\", \n     border = \"black\", \n     breaks = 20)\n\n# 2. Vertical Barchart for CO2 Footprint of Top 10 Countries by Population\ntop10_pop &lt;- happiness_data[order(-happiness_data$`Population (thousands)`), ][1:10, ]\nbarplot(top10_pop$`Carbon Footprint (tCO2e)`, \n        names.arg = top10_pop$Country, \n        col = \"lightgreen\", \n        main = \"Top 10 Countries by CO2 Footprint\", \n        ylab = \"CO2 Footprint (tCO2e)\", \n        las = 2)  # Rotate x-axis labels for better readability\n\n\n# Group 2: Horizontal Barchart and Pie Chart\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 3. Horizontal Barchart for GDP per capita of Top 10 Countries by Population\nbarplot(top10_pop$`GDP per capita ($)`, \n        names.arg = top10_pop$Country, \n        col = \"coral\", \n        main = \"Top 10 Countries by GDP per Capita\", \n        xlab = \"GDP per Capita ($)\", \n        horiz = TRUE)  # Horizontal bar plot\n\n# 4. Pie Chart of Continent Proportions\ncontinent_count &lt;- table(happiness_data$Continent)\npie(continent_count, \n    main = \"Proportion of Countries by Continent\", \n    col = rainbow(length(continent_count)))\n\n\n# Group 3: Boxplot and Scatterplot\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 5. Boxplot of Wellbeing (Ladder of Life) by Continent\nboxplot(happiness_data$`Ladder of life (Wellbeing) (0-10)` ~ happiness_data$Continent,\n        col = \"lightpink\", \n        main = \"Wellbeing by Continent\", \n        ylab = \"Ladder of Life (Wellbeing)\", \n        xlab = \"Continent\")\n\n# 6. Scatterplot of GDP per capita vs HPI (Happy Planet Index)\nplot(happiness_data$`GDP per capita ($)`, happiness_data$HPI, \n     col = \"blue\", \n     pch = 16, \n     xlab = \"GDP per Capita ($)\", \n     ylab = \"Happy Planet Index (HPI)\", \n     main = \"GDP per Capita vs HPI\")\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n###Question 2\nlibrary (readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nhappiness_data &lt;- read_excel(\"C:/Users/pande/OneDrive - The University of Texas at Dallas/PhD/2024_Fall/Data_Visualization/Happiness.xlsx\")\n\n# Group 1: Histogram and Vertical Barchart\n\n# 1. Histogram of Life Expectancy\nhist_plot &lt;- ggplot(happiness_data, aes(x = `Life Expectancy (years)`)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20) +\n  labs(title = \"Histogram of Life Expectancy\", x = \"Life Expectancy (years)\", y = \"Count\") +\n  theme_minimal()\n\n# 2. Vertical Barchart for CO2 Footprint of Top 10 Countries by Population\ntop10_pop &lt;- happiness_data %&gt;% arrange(desc(`Population (thousands)`)) %&gt;% slice(1:10)\n\nbar_plot &lt;- ggplot(top10_pop, aes(x = reorder(Country, `Carbon Footprint (tCO2e)`), y = `Carbon Footprint (tCO2e)`)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Top 10 Countries by CO2 Footprint\", x = \"Country\", y = \"CO2 Footprint (tCO2e)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels\n\n# Show the two plots side by side using gridExtra\nlibrary(gridExtra)\ngrid.arrange(hist_plot, bar_plot, nrow = 2)\n\n\n# Group 2: Horizontal Barchart and Pie Chart\n\n# 3. Horizontal Barchart for GDP per capita of Top 10 Countries by Population\nhbar_plot &lt;- ggplot(top10_pop, aes(x = `GDP per capita ($)`, y = reorder(Country, `GDP per capita ($)`))) +\n  geom_bar(stat = \"identity\", fill = \"coral\") +\n  labs(title = \"Top 10 Countries by GDP per Capita\", x = \"GDP per Capita ($)\", y = \"Country\") +\n  theme_minimal()\n\n# 4. Pie Chart of Continent Proportions\ncontinent_count &lt;- happiness_data %&gt;% count(Continent)\n\npie_plot &lt;- ggplot(continent_count, aes(x = \"\", y = n, fill = Continent)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\") +\n  labs(title = \"Proportion of Countries by Continent\") +\n  theme_minimal()\n\n# Show the two plots side by side using gridExtra\ngrid.arrange(hbar_plot, pie_plot, nrow = 2)\n\n\n# Group 3: Boxplot and Scatterplot\n\n# 5. Boxplot of Wellbeing (Ladder of Life) by Continent\nboxplot_plot &lt;- ggplot(happiness_data, aes(x = Continent, y = `Ladder of life (Wellbeing) (0-10)`)) +\n  geom_boxplot(fill = \"lightpink\") +\n  labs(title = \"Wellbeing by Continent\", x = \"Continent\", y = \"Ladder of Life (Wellbeing)\") +\n  theme_minimal()\n\n# 6. Scatterplot of GDP per capita vs HPI (Happy Planet Index)\nscatter_plot &lt;- ggplot(happiness_data, aes(x = `GDP per capita ($)`, y = HPI)) +\n  geom_point(color = \"blue\", size = 3) +\n  labs(title = \"GDP per Capita vs HPI\", x = \"GDP per Capita ($)\", y = \"Happy Planet Index (HPI)\") +\n  theme_minimal()\n\n# Show the two plots side by side using gridExtra\ngrid.arrange(boxplot_plot, scatter_plot, nrow = 2)\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n\n\n\nOutput"
  },
  {
    "objectID": "Assignment5.html#question-1",
    "href": "Assignment5.html#question-1",
    "title": "Assignment 5",
    "section": "",
    "text": "library (readxl)\nhappiness_data &lt;- read_excel(\"C:/Users/pande/OneDrive - The University of Texas at Dallas/PhD/2024_Fall/Data_Visualization/Happiness.xlsx\")\n\n# Group 1: Histogram and Vertical Barchart\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 1. Histogram of Life Expectancy\nhist(happiness_data$`Life Expectancy (years)`, \n     col = \"lightblue\", \n     main = \"Histogram of Life Expectancy\", \n     xlab = \"Life Expectancy (years)\", \n     border = \"black\", \n     breaks = 20)\n\n# 2. Vertical Barchart for CO2 Footprint of Top 10 Countries by Population\ntop10_pop &lt;- happiness_data[order(-happiness_data$`Population (thousands)`), ][1:10, ]\nbarplot(top10_pop$`Carbon Footprint (tCO2e)`, \n        names.arg = top10_pop$Country, \n        col = \"lightgreen\", \n        main = \"Top 10 Countries by CO2 Footprint\", \n        ylab = \"CO2 Footprint (tCO2e)\", \n        las = 2)  # Rotate x-axis labels for better readability\n\n\n# Group 2: Horizontal Barchart and Pie Chart\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 3. Horizontal Barchart for GDP per capita of Top 10 Countries by Population\nbarplot(top10_pop$`GDP per capita ($)`, \n        names.arg = top10_pop$Country, \n        col = \"coral\", \n        main = \"Top 10 Countries by GDP per Capita\", \n        xlab = \"GDP per Capita ($)\", \n        horiz = TRUE)  # Horizontal bar plot\n\n# 4. Pie Chart of Continent Proportions\ncontinent_count &lt;- table(happiness_data$Continent)\npie(continent_count, \n    main = \"Proportion of Countries by Continent\", \n    col = rainbow(length(continent_count)))\n\n\n# Group 3: Boxplot and Scatterplot\n\n# Set layout to 2 rows, 1 column (2x1)\npar(mfrow = c(2, 1))\n\n# 5. Boxplot of Wellbeing (Ladder of Life) by Continent\nboxplot(happiness_data$`Ladder of life (Wellbeing) (0-10)` ~ happiness_data$Continent,\n        col = \"lightpink\", \n        main = \"Wellbeing by Continent\", \n        ylab = \"Ladder of Life (Wellbeing)\", \n        xlab = \"Continent\")\n\n# 6. Scatterplot of GDP per capita vs HPI (Happy Planet Index)\nplot(happiness_data$`GDP per capita ($)`, happiness_data$HPI, \n     col = \"blue\", \n     pch = 16, \n     xlab = \"GDP per Capita ($)\", \n     ylab = \"Happy Planet Index (HPI)\", \n     main = \"GDP per Capita vs HPI\")\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n###Question 2\nlibrary (readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nhappiness_data &lt;- read_excel(\"C:/Users/pande/OneDrive - The University of Texas at Dallas/PhD/2024_Fall/Data_Visualization/Happiness.xlsx\")\n\n# Group 1: Histogram and Vertical Barchart\n\n# 1. Histogram of Life Expectancy\nhist_plot &lt;- ggplot(happiness_data, aes(x = `Life Expectancy (years)`)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20) +\n  labs(title = \"Histogram of Life Expectancy\", x = \"Life Expectancy (years)\", y = \"Count\") +\n  theme_minimal()\n\n# 2. Vertical Barchart for CO2 Footprint of Top 10 Countries by Population\ntop10_pop &lt;- happiness_data %&gt;% arrange(desc(`Population (thousands)`)) %&gt;% slice(1:10)\n\nbar_plot &lt;- ggplot(top10_pop, aes(x = reorder(Country, `Carbon Footprint (tCO2e)`), y = `Carbon Footprint (tCO2e)`)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Top 10 Countries by CO2 Footprint\", x = \"Country\", y = \"CO2 Footprint (tCO2e)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels\n\n# Show the two plots side by side using gridExtra\nlibrary(gridExtra)\ngrid.arrange(hist_plot, bar_plot, nrow = 2)\n\n\n# Group 2: Horizontal Barchart and Pie Chart\n\n# 3. Horizontal Barchart for GDP per capita of Top 10 Countries by Population\nhbar_plot &lt;- ggplot(top10_pop, aes(x = `GDP per capita ($)`, y = reorder(Country, `GDP per capita ($)`))) +\n  geom_bar(stat = \"identity\", fill = \"coral\") +\n  labs(title = \"Top 10 Countries by GDP per Capita\", x = \"GDP per Capita ($)\", y = \"Country\") +\n  theme_minimal()\n\n# 4. Pie Chart of Continent Proportions\ncontinent_count &lt;- happiness_data %&gt;% count(Continent)\n\npie_plot &lt;- ggplot(continent_count, aes(x = \"\", y = n, fill = Continent)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\") +\n  labs(title = \"Proportion of Countries by Continent\") +\n  theme_minimal()\n\n# Show the two plots side by side using gridExtra\ngrid.arrange(hbar_plot, pie_plot, nrow = 2)\n\n\n# Group 3: Boxplot and Scatterplot\n\n# 5. Boxplot of Wellbeing (Ladder of Life) by Continent\nboxplot_plot &lt;- ggplot(happiness_data, aes(x = Continent, y = `Ladder of life (Wellbeing) (0-10)`)) +\n  geom_boxplot(fill = \"lightpink\") +\n  labs(title = \"Wellbeing by Continent\", x = \"Continent\", y = \"Ladder of Life (Wellbeing)\") +\n  theme_minimal()\n\n# 6. Scatterplot of GDP per capita vs HPI (Happy Planet Index)\nscatter_plot &lt;- ggplot(happiness_data, aes(x = `GDP per capita ($)`, y = HPI)) +\n  geom_point(color = \"blue\", size = 3) +\n  labs(title = \"GDP per Capita vs HPI\", x = \"GDP per Capita ($)\", y = \"Happy Planet Index (HPI)\") +\n  theme_minimal()\n\n# Show the two plots side by side using gridExtra\ngrid.arrange(boxplot_plot, scatter_plot, nrow = 2)\n\n\n\nOutput\n\n\n\n\n\nOutput\n\n\n\n\n\nOutput"
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "##This is the example of shiny"
  }
]